# ğŸ“¦ ETL de Ventas con SSIS + Python + SQL Server

En este apartado, se documenta el flujo ETL implementado con Visual Studio con un paquete **SSIS**, complementado con generaciÃ³n automÃ¡tica de ventas mediante **Python**, y organizado en un flujo **SRC â†’ STG â†’ DW** para garantizar calidad, integridad y trazabilidad de los datos.

Se incluyen etiquetas (A, B, C, D, E y F) para relacionar el diagrama anotado con la descripciÃ³n tÃ©cnica.

---

# ğŸ“˜ 1. DescripciÃ³n General

Este proyecto implementa un **pipeline ETL completo** para procesar informaciÃ³n comercial (ventas, productos y maestros) usando:

* **Python** para generar automÃ¡ticamente ventas diarias (SRC), realizazr la exploraciÃ³n, anÃ¡lisis de calidad, validaciÃ³n y refinamiento de datos (STG).
* **SQL Server** como base de datos para las capas Source (SRC), Staging (STG) y Data Warehouse (DW).
* **SSIS** para ejecutar el flujo de extracciÃ³n, transformaciÃ³n y carga.

El objetivo es automatizar la ingesta de ventas y preparar datos limpios y consistentes para reporting y anÃ¡lisis.

---

# ğŸ—ºï¸ 2. Arquitectura del ETL

A continuaciÃ³n la estructura conceptual del pipeline, dividida en Ã¡reas clave.

```
A â†’ Productos
B â†’ Maestros
C â†’ GeneraciÃ³n de Ventas (Python)
D â†’ Capa SRC
E â†’ Capa STG
F â†’ Capa DW
```

Cada bloque corresponde a un *Sequence Container* o secciÃ³n especÃ­fica del paquete SSIS.

---

# ğŸŸ¦ A. Procesamiento de Productos

Este contenedor administra tablas maestras relacionadas al catÃ¡logo de productos.

Incluye:

* Familia de Producto
* Rubro de Producto
* Tipo de Producto
* Producto

Proceso para cada entidad:

1. **Login_src** â†’ Obtiene datos desde la fuente (CSV/ERP).
2. **Carga DW** â†’ Inserta o actualiza en la tabla del Data Warehouse.
3. **ValidaciÃ³n** â†’ Revisa conteos e integridad.

Este mÃ³dulo asegura que las ventas se puedan relacionar correctamente con la dimensiÃ³n producto.

---

# ğŸŸ§ B. Procesamiento de Maestros

Contiene entidades base usadas en ventas:

* Cliente
* Vendedor
* Tienda
* Ubigeo

Flujo para cada maestro:

1. Registro desde SRC.
2. Carga al DW.
3. ValidaciÃ³n.

Estas entidades deben procesarse antes del bloque de ventas para garantizar integridad en claves forÃ¡neas.

---

# ğŸŸ© C. GeneraciÃ³n AutomÃ¡tica de Ventas (Python) e Ingesta a Capa SRC

Previo al ETL principal, se ejecutan dos scripts Python encargados de generar ventas diarias:

* `Generar_VentasCabecera.py`
* `Generar_VentasDetalle.py`

Ambos crean archivos diarios (cabecera y detalle), simulando transacciones reales.

Al finalizar la creaciÃ³n de las nuevas transacciones, estas son imputadas sin transformaciÃ³n a las tablas `fact_VentaCabecera` y `fact_VentaDetalle` en la BD SRC_Spherzone en SQL Server.

(Estos archivos son la entrada del flujo de ventas)

Ventajas:

* Mantener trazabilidad del dato crudo.
* Poder reejecutar el ETL si fuera necesario.

---

# ğŸŸ¨ D. TransformaciÃ³n en STG

En esta capa se depura y valida la informaciÃ³n:

* ConversiÃ³n de tipos de datos.
* Control de nulos.
* ValidaciÃ³n de claves forÃ¡neas (cliente, vendedor, producto, tienda).
* Integridad cabecera-detalle.

Incluye pasos de **refinamiento**, donde se aplican reglas de negocio.

---

# ğŸŸª E. Carga al Data Warehouse (DW)

Una vez validados y refinados los datos:

* Se insertan en `fact_VentaCabecera` y `fact_VentaDetalle` en la BD DW_Spherzone en SQL Server.
* Se realizan merge, actualizaciones y creaciÃ³n de registros nuevos.

Estas tablas son consumidas por sistemas de BI como Power BI.

---


# ğŸ–¼ï¸ 3. Diagrama del ETL

A continuaciÃ³n se muestra la el diagrama en Visual Studio seÃ±alando cada una de las fases (Aâ€“F) del proceso ETL.


<img width="1811" height="1007" alt="diseÃ±o_paquete_etl" src="https://github.com/user-attachments/assets/f5461072-221a-4b3e-82df-4d4bf0cc8435" />


---

# ğŸ“Œ 4. Consideraciones TÃ©cnicas

* Se recomienda ejecutar el ETL mediante SQL Server Agent (Jobs diarios).
* Los scripts Python pueden integrarse con paquetes SSIS mediante "Execute Process Task".
* La capa STG actÃºa como control de calidad para evitar anomalÃ­as.
* Las validaciones finales garantizan consistencia antes de cargar el DW.

---

# ğŸš€ 5. PrÃ³ximas Mejoras

* Automatizar envÃ­os de alertas por correo.
* MigraciÃ³n del ETL a Azure Data Factory o Prefect.
* Implementar pruebas unitarias para transformaciÃ³n de datos.

---

# âœ¨ Autor

Desarrollado por **Franco Telenta Chavez**, Bach. en IngenierÃ­a Industrial especializado en anÃ¡lisis de datos y automatizaciÃ³n ETL.

